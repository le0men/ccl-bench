# workload_template.yaml
version: 1

description: >
  Llama3 8B training on 4 GPUs, TP=1, PP=1, DP(shard)=1. Torch distributed
  runtime, NCCL comms over Slingshot/RoCEv2. No compile/fusion; selective
  activation checkpointing.
  We modified the original train.py code to collect tracing information. 

hf_url: https://huggingface.co/meta-llama/Llama-3.1-8B
trace_url: (in the Cornell Box)
workload:
  model:
    phase: training
    moe: false
    granularity: model_fwd_bwd_pass
    model_family: llama-3.1-8b
    precision: bf16
  data:
    batch_size: 4
    seq_len: 8192
    dataset: c4
  hardware:
    network_topo:
      topology: slingshot
      bandwidth_gbps:
        - 200              # TODO adjust scale-out
        - 2000             # TODO adjust scale-up
    xpu_spec:
      type: GPU
      model: nvidia_a100
      total_count: 16
      count_per_node: 4
    driver_version: cuda_12.4
  
Model-executor:
  framework: 
    name: PyTorch
    compiler_tool_selection: plain_pytorch
  model_plan_parallelization: 
    dp_replicate: 1
    dp_shard: 1
    tp: 1
    pp: 1
    cp: 1
  communication_library:
    name: NCCL
    env:
      NCCL_IB_QPS_PER_CONNECTION: "4" # TODO verify, add more envs as needed
  protocol_selection:
    - rocev2
    - p2p
  


metric_source:
  traces:
    - kineto_trace
